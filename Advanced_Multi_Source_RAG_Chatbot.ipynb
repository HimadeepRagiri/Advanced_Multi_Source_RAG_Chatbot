{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "52bd1fada8df4860840002a67270ab60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fbe816da57f945febc50875be97ae64f",
              "IPY_MODEL_692a67c93c5947b596b5d61a1ea0f294",
              "IPY_MODEL_8df303c29cc54c7c9cddedbc046a37d0"
            ],
            "layout": "IPY_MODEL_9b7b1813356343f488347bfa750a4c8d"
          }
        },
        "fbe816da57f945febc50875be97ae64f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ddefd9ce4d7498589570dc4653ec7a4",
            "placeholder": "​",
            "style": "IPY_MODEL_0c41a047e79b4c78a85abb231090578d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "692a67c93c5947b596b5d61a1ea0f294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_255263bb0a9a4afb8ce96e29579eb4dd",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f5f0aa7914c40a2b53585bdb66f93bf",
            "value": 2
          }
        },
        "8df303c29cc54c7c9cddedbc046a37d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3da846499314b19a9d523e90a3e64c5",
            "placeholder": "​",
            "style": "IPY_MODEL_5c00c4ef58e84629862cdbf602913109",
            "value": " 2/2 [01:29&lt;00:00, 41.72s/it]"
          }
        },
        "9b7b1813356343f488347bfa750a4c8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ddefd9ce4d7498589570dc4653ec7a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c41a047e79b4c78a85abb231090578d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "255263bb0a9a4afb8ce96e29579eb4dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f5f0aa7914c40a2b53585bdb66f93bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3da846499314b19a9d523e90a3e64c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c00c4ef58e84629862cdbf602913109": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.tools import TavilySearchResults\n",
        "import gradio as gr\n",
        "import wikipedia\n",
        "import yt_dlp\n",
        "from datetime import datetime\n",
        "import os\n"
      ],
      "metadata": {
        "id": "UZRr5HuPy_Gn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Quantized LLM (4-bit Mistral)\n",
        "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "quant_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "52bd1fada8df4860840002a67270ab60",
            "fbe816da57f945febc50875be97ae64f",
            "692a67c93c5947b596b5d61a1ea0f294",
            "8df303c29cc54c7c9cddedbc046a37d0",
            "9b7b1813356343f488347bfa750a4c8d",
            "4ddefd9ce4d7498589570dc4653ec7a4",
            "0c41a047e79b4c78a85abb231090578d",
            "255263bb0a9a4afb8ce96e29579eb4dd",
            "8f5f0aa7914c40a2b53585bdb66f93bf",
            "a3da846499314b19a9d523e90a3e64c5",
            "5c00c4ef58e84629862cdbf602913109"
          ]
        },
        "id": "5L-279cIz_WN",
        "outputId": "9ac66713-602b-4eae-eab0-4b7a0696d55c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52bd1fada8df4860840002a67270ab60"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Embedding Model\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Load PDFs into FAISS Vector Store\n",
        "def load_documents(pdf_path):\n",
        "    loader = PyMuPDFLoader(pdf_path)\n",
        "    documents = loader.load()\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    texts = splitter.split_documents(documents)\n",
        "    return FAISS.from_documents(texts, embedding_model)\n",
        "\n",
        "# Web Search Tool\n",
        "search_tool = TavilySearchResults()\n",
        "\n",
        "# Wikipedia Retrieval\n",
        "def search_wikipedia(query):\n",
        "    try:\n",
        "        return wikipedia.summary(query, sentences=2)\n",
        "    except:\n",
        "        return \"No relevant Wikipedia information found.\"\n",
        "\n",
        "# YouTube Transcription Retrieval\n",
        "def get_youtube_transcript(url):\n",
        "    if url:  # Only process if URL is provided\n",
        "        ydl_opts = {\n",
        "            \"quiet\": True,\n",
        "            \"format\": \"bestaudio/best\",\n",
        "            \"cookiefile\": \"/content/cookies.txt\"  # Use the exported cookies\n",
        "        }\n",
        "        try:\n",
        "            with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "                info = ydl.extract_info(url, download=False)\n",
        "                transcript = info.get(\"description\", \"\")\n",
        "            return transcript if transcript else \"No transcript available.\"\n",
        "        except Exception as e:\n",
        "            return f\"Error retrieving transcript: {str(e)}\"\n",
        "    return \"\"\n"
      ],
      "metadata": {
        "id": "pbJEze6u0udN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4246c2b8-bc61-4e8e-9d59-f2b0e89a1faa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-c9a7a344ab23>:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversation Memory\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "def update_memory(query, response):\n",
        "    \"\"\"Stores conversation history\"\"\"\n",
        "    memory.save_context({\"input\": query}, {\"output\": response})\n"
      ],
      "metadata": {
        "id": "tYhjziu40uaw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bac673bd-3e46-4fd1-8422-e41bbc7410dd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-89b90bc97acf>:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(return_messages=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve Top Sources\n",
        "def retrieve_sources(query, vector_db=None, youtube_url=None):\n",
        "    sources = \"\"\n",
        "\n",
        "    # Document search if PDF is loaded\n",
        "    if vector_db is not None:\n",
        "        retrieved_docs = vector_db.similarity_search(query, k=10)\n",
        "        ranked_docs = [doc.page_content for doc in retrieved_docs]\n",
        "        sources += f\"Documents: {ranked_docs}\\n\"\n",
        "\n",
        "    # Web search\n",
        "    web_search = search_tool.run(query)\n",
        "    sources += f\"Web: {web_search}\\n\"\n",
        "\n",
        "    # Wikipedia\n",
        "    wiki_summary = search_wikipedia(query)\n",
        "    sources += f\"Wikipedia: {wiki_summary}\\n\"\n",
        "\n",
        "    # YouTube transcript if URL provided\n",
        "    if youtube_url:\n",
        "        transcript = get_youtube_transcript(youtube_url)\n",
        "        sources += f\"YouTube Transcript: {transcript}\"\n",
        "\n",
        "    return sources\n",
        "\n",
        "# Generate Response with LLM\n",
        "def generate_response(query, vector_db=None, youtube_url=None):\n",
        "    # Retrieve sources\n",
        "    sources = retrieve_sources(query, vector_db=vector_db, youtube_url=youtube_url)\n",
        "\n",
        "    # Construct the prompt\n",
        "    prompt = f\"\"\"\n",
        "    You are a helpful and intelligent assistant that must answer questions using only the information provided in the sources below.\n",
        "\n",
        "    --------------------\n",
        "    Sources:\n",
        "    {sources}\n",
        "    --------------------\n",
        "\n",
        "    Using only the above sources, provide a clear, direct, and concise answer to the following question. If the sources do not contain sufficient information to answer the question, respond with \"Insufficient information provided.\"\n",
        "\n",
        "    Question: {query}\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize and move to GPU\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Get the length of the input prompt\n",
        "    input_length = inputs['input_ids'].shape[1]\n",
        "\n",
        "    # Generate the response\n",
        "    output = model.generate(**inputs, max_new_tokens=200)\n",
        "\n",
        "    # Extract only the generated tokens (exclude the prompt)\n",
        "    generated_ids = output[0, input_length:]\n",
        "    response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Update memory with query and clean answer\n",
        "    update_memory(query, response)\n",
        "\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "2pg_lQ_t0uVP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference function\n",
        "def inference():\n",
        "    # Sample PDF path\n",
        "    pdf_path = \"None\"\n",
        "    vector_db = None\n",
        "\n",
        "    if os.path.exists(pdf_path):\n",
        "        print(\"Loading PDF...\")\n",
        "        vector_db = load_documents(pdf_path)\n",
        "        print(\"PDF loaded successfully!\")\n",
        "    else:\n",
        "        print(\"No PDF found, proceeding without document search.\")\n",
        "\n",
        "    # Example Query\n",
        "    query = \"What is Low Rank Adaptation in the context of machine learning models?\"\n",
        "    youtube_url = None\n",
        "\n",
        "    print(\"\\nQuery:\", query)\n",
        "    print(\"YouTube URL:\", youtube_url if youtube_url else \"None\")\n",
        "\n",
        "    # source retrieval\n",
        "    print(\"\\nRetrieving sources...\")\n",
        "    sources = retrieve_sources(query, vector_db=vector_db, youtube_url=youtube_url)\n",
        "    print(\"Sources retrieved:\")\n",
        "    print(sources)\n",
        "\n",
        "    # response generation\n",
        "    print(\"\\nGenerating response...\")\n",
        "    response = generate_response(query, vector_db=vector_db, youtube_url=youtube_url)\n",
        "    print(\"\\nResponse:\")\n",
        "    print(response)\n",
        "\n",
        "    # Check memory\n",
        "    print(\"\\nConversation Memory:\")\n",
        "    memory_content = memory.load_memory_variables({})\n",
        "    for msg in memory_content[\"history\"]:\n",
        "        if msg.type == \"human\":\n",
        "            print(f\"Q: {msg.content}\")\n",
        "        elif msg.type == \"ai\":\n",
        "            print(f\"A: {msg.content}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    inference()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4x1LxL8JJAw",
        "outputId": "2f725105-0c0d-40e5-e2f3-5c6443de1f34"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No PDF found, proceeding without document search.\n",
            "\n",
            "Query: What is Low Rank Adaptation in the context of machine learning models?\n",
            "YouTube URL: None\n",
            "\n",
            "Retrieving sources...\n",
            "Sources retrieved:\n",
            "Web: [{'title': 'What is LoRA? | Low-rank adaptation - Cloudflare', 'url': 'https://www.cloudflare.com/learning/ai/what-is-lora/', 'content': 'Low-rank adaptation, or LoRA, is a less expensive, more efficient method for adapting large machine learning models to specific uses. Learn how LoRA works.', 'score': 0.9438932}, {'title': 'Low Rank Adaptation: A Technical deep dive - ML6', 'url': 'https://www.ml6.eu/blogpost/low-rank-adaptation-a-technical-deep-dive', 'content': 'Enter LoRA (Low Rank Adaptation) — a groundbreaking and efficient fine-tuning technique that harnesses the power of these advanced models for custom tasks and datasets without straining resources or incurring excessive costs. LoRA is an efficient finetuning technique proposed by Microsoft researchers to adapt large models to specific tasks and datasets. The main idea behind LoRA is that the change in weights during model adaptation also has a low intrinsic rank/dimension. Finetuning the model using LoRA with r=32 (where r is the rank of the update matrix) reduces the number of tunable parameters to 15.7 million, which is 1% of the parameters of the entire model.', 'score': 0.9325087}, {'title': 'What is LoRA (Low-Rank Adaption)? - IBM', 'url': 'https://www.ibm.com/think/topics/lora', 'content': 'LoRA leverages the concept of lower-rank matrices to make the model training process extremely efficient and fast. By leveraging smaller matrices, which are called low-rank matrices, LoRA adapts these models effectively. LoRA adds low-rank matrices to the frozen original machine learning model. This significantly reduces the trainable parameters of the model and the GPU memory requirement for the training process, which is another significant challenge when it comes to fine-tuning or training large models. To implement LoRA fine tuning with HuggingFace using Python and PyTorch, developers can use the parameter-efficient fine-tuning (PEFT) library to inject the LoRA adapters into the model and use them as the update matrices. Removing this resiliency can make models less accurate, so the rank of update matrices can be tuned as a part of the LoRA process.', 'score': 0.8825193}, {'title': 'LoRA: Low-Rank Adaptation of Large Language Models - arXiv', 'url': 'https://arxiv.org/abs/2106.09685', 'content': 'cs arXiv:2106.09685 LoRA: Low-Rank Adaptation of Large Language Models Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. Subjects:   Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as:    arXiv:2106.09685 [cs.CL] (or arXiv:2106.09685v2 [cs.CL] for this version) cs Bibliographic and Citation Tools Connected Papers Toggle', 'score': 0.853046}, {'title': 'Mastering Low-Rank Adaptation (LoRA): Enhancing Large ...', 'url': 'https://www.datacamp.com/tutorial/mastering-low-rank-adaptation-lora-enhancing-large-language-models-for-efficient-adaptation', 'content': 'Discover how LoRA revolutionizes the fine-tuning of Large Language Models. LoRA: Low-Rank Adaptation of Large Language Models Unlike traditional fine-tuning that requires adjusting the entire model, LoRA focuses on modifying a smaller subset of parameters (lower-rank matrices), thereby reducing computational and memory overhead. By requiring fewer trainable parameters, LoRA makes it feasible to fine-tune large models on less powerful hardware. Instead of fully fine-tuning large models like Stable Diffusion, we only train lower-rank matrices on small datasets. By leveraging lower-rank matrices, LoRA offers a more efficient and cost-effective approach to model adaptation, significantly reducing the trainable parameters and GPU memory requirements, thus enabling faster training and memory efficiency.', 'score': 0.8190992}]\n",
            "Wikipedia: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.\n",
            "\n",
            "\n",
            "Generating response...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response:\n",
            "--------------------\n",
            "\n",
            "    Answer:\n",
            "\n",
            "    Low Rank Adaptation (LoRA) is a technique used in machine learning models to reduce the number of trainable parameters while maintaining the model's performance. It involves adding low-rank matrices to the model's parameters, which reduces the number of trainable parameters and the GPU memory requirement for the training process.\n",
            "\n",
            "    LoRA leverages the concept of lower-rank matrices to make the model training process extremely efficient and fast. By leveraging smaller matrices, which are called low-rank matrices, LoRA adapts these models effectively.\n",
            "\n",
            "    LoRA adds low-rank matrices to the frozen original machine learning model. This significantly reduces the trainable parameters of the model and the GPU memory requirement for the training process, which is another significant challenge when it comes to fine-tuning or training large models.\n",
            "\n",
            "    To implement LoRA fine tuning with HuggingFace using Python and PyTorch,\n",
            "\n",
            "Conversation Memory:\n",
            "Q: What is Low Rank Adaptation in the context of machine learning models?\n",
            "A: --------------------\n",
            "\n",
            "    Answer:\n",
            "\n",
            "    Low Rank Adaptation (LoRA) is a technique used in machine learning models to reduce the number of trainable parameters while maintaining the model's performance. It involves adding low-rank matrices to the model's parameters, which reduces the number of trainable parameters and the GPU memory requirement for the training process.\n",
            "\n",
            "    LoRA leverages the concept of lower-rank matrices to make the model training process extremely efficient and fast. By leveraging smaller matrices, which are called low-rank matrices, LoRA adapts these models effectively.\n",
            "\n",
            "    LoRA adds low-rank matrices to the frozen original machine learning model. This significantly reduces the trainable parameters of the model and the GPU memory requirement for the training process, which is another significant challenge when it comes to fine-tuning or training large models.\n",
            "\n",
            "    To implement LoRA fine tuning with HuggingFace using Python and PyTorch,\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio Web Deployment\n",
        "def gradio_interface(query, youtube_url, pdf):\n",
        "    \"\"\"\n",
        "    This function is the Gradio callback.\n",
        "    It checks if a PDF is uploaded, loads it into the vector store,\n",
        "    and then generates a response based on the provided query, YouTube URL, and PDF.\n",
        "    It also returns the full chat history.\n",
        "    \"\"\"\n",
        "    vector_db = None\n",
        "    # If a PDF is provided, load it into FAISS\n",
        "    if pdf is not None:\n",
        "        # Gradio's File component returns a dict when type=\"binary\"\n",
        "        if isinstance(pdf, dict):\n",
        "            pdf_path = pdf.get(\"name\", \"uploaded_pdf.pdf\")\n",
        "            with open(pdf_path, \"wb\") as f:\n",
        "                f.write(pdf[\"data\"])\n",
        "        elif isinstance(pdf, str):\n",
        "            pdf_path = pdf\n",
        "        else:\n",
        "            pdf_path = None\n",
        "\n",
        "        if pdf_path is not None and os.path.exists(pdf_path):\n",
        "            vector_db = load_documents(pdf_path)\n",
        "\n",
        "    # Generate response using the provided inputs\n",
        "    response = generate_response(query, vector_db=vector_db, youtube_url=youtube_url)\n",
        "\n",
        "    # Retrieve conversation history\n",
        "    history = memory.load_memory_variables({})[\"history\"]\n",
        "    chat_history = \"\"\n",
        "    for msg in history:\n",
        "        # Check for common attributes in stored messages.\n",
        "        if hasattr(msg, \"role\"):\n",
        "            if msg.role == \"human\":\n",
        "                chat_history += f\"User: {msg.content}\\n\"\n",
        "            elif msg.role == \"ai\":\n",
        "                chat_history += f\"Assistant: {msg.content}\\n\"\n",
        "        elif hasattr(msg, \"type\"):\n",
        "            if msg.type == \"human\":\n",
        "                chat_history += f\"User: {msg.content}\\n\"\n",
        "            elif msg.type == \"ai\":\n",
        "                chat_history += f\"Assistant: {msg.content}\\n\"\n",
        "        else:\n",
        "            chat_history += f\"{msg}\\n\"\n",
        "\n",
        "    return response, chat_history\n",
        "\n",
        "# Build Gradio interface\n",
        "with gr.Blocks() as app:\n",
        "    gr.Markdown(\"# Advanced RAG Chatbot with PDF, YouTube & Web Search\")\n",
        "    with gr.Row():\n",
        "         query_input = gr.Textbox(label=\"Enter your question\", placeholder=\"Type your question here...\", lines=2)\n",
        "         youtube_input = gr.Textbox(label=\"Enter YouTube URL (optional)\", placeholder=\"YouTube URL here...\", lines=1)\n",
        "\n",
        "    pdf_input = gr.File(label=\"Upload PDF (optional)\", file_count=\"single\", type=\"binary\")\n",
        "    generate_button = gr.Button(\"Generate Response\")\n",
        "    response_output = gr.Textbox(label=\"Response\", lines=10)\n",
        "    chat_history_output = gr.Textbox(label=\"Chat History\", lines=10)\n",
        "\n",
        "    generate_button.click(\n",
        "        fn=gradio_interface,\n",
        "        inputs=[query_input, youtube_input, pdf_input],\n",
        "        outputs=[response_output, chat_history_output]\n",
        "    )\n",
        "\n",
        "# launch the interface\n",
        "app.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "dizyhcBhPEtE",
        "outputId": "f32c758a-60f0-4fdf-e535-643ec3942cb7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://cd27c0c9405c7074c4.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cd27c0c9405c7074c4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "neMTsmITPEVU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}